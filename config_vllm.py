\"\"\"Central configuration for vLLM runtime settings.\n+\n+Adjust these values per deployment (e.g., AWS instance shape). The Agent\n+constructor can still override them programmatically if needed.\n+\"\"\"\n+\n+VLLM_CONFIG = {\n+    # Number of GPUs to shard the model across (tensor parallel). If None, use all visible GPUs.\n+    \"tensor_parallel_size\": None,\n+    # Pipeline parallel partitions (use >1 for multi-node or very large models).\n+    \"pipeline_parallel_size\": 1,\n+    # Backend for distributed executor: \"mp\" (default) or \"ray\".\n+    \"distributed_executor_backend\": \"mp\",\n+    # Fraction of each GPU's memory vLLM may use (0-1).\n+    \"gpu_memory_utilization\": 0.90,\n+    # Host swap space in GB to offload KV cache (helps with large prompts/models).\n+    \"swap_space_gb\": None,\n+    # Optional hard cap on context length; if None, let vLLM infer from model.\n+    \"max_model_len\": None,\n+}\n+\n*** End Patch" ***!
