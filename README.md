# Forecaster (vLLM edition)\n\nLightweight forecasting ensemble that runs locally or on multi‑GPU AWS instances using vLLM. The torch subpackage defaults to vLLM with Run:ai S3 streaming for large models.\n\n## Repository layout\n- `torch/agents_vllm.py` – vLLM-based agent (forecasts + optional logic model).\n- `torch/ensemble_torch.py` – orchestrates multiple agents and stores results.\n- `torch/config_vllm.py` – single source of truth for model paths and vLLM/Run:ai settings.\n\n## Quick start (local single box)\n```bash\ncd torch\npip install -r requirements.txt\npython forecaster_torch.py\n```\nDefaults load smaller HF models for local testing.\n\n## Configure models and parallelism\nEdit `torch/config_vllm.py`:\n- Set `FORECAST_MODEL_PATHS_VLLM` to your model URIs (HF or S3).\n- Tune `VLLM_CONFIG` (`tensor_parallel_size`, `pipeline_parallel_size`, `distributed_executor_backend`, `gpu_memory_utilization`, `load_format`, `model_loader_extra_config`).\n\n## Deploying on AWS EC2 with vLLM + Run:ai S3 streaming\nThis repo is configured to run large models on multi-GPU AWS instances using vLLM and the Run:ai streamer to pull weights directly from S3.\n\n### Prereqs\n- Multi-GPU EC2 (e.g., p4d/p5) with CUDA/NCCL working.\n- IAM role on the instance with read access to your S3 model bucket.\n- Python 3.10+.\n\nInstall deps:\n```bash\npip install -r requirements.txt\n```\n`vllm[runai]` is pinned to a stable 0.10.x release for Run:ai support.\n\n### Configure model paths and loader\nEdit `torch/config_vllm.py`:\n- Set `FORECAST_MODEL_PATHS_VLLM` entries to your S3 URIs (e.g., `s3://your-bucket/Llama-3-70B`).\n- Keep `VLLM_CONFIG.load_format` as `runai_streamer`.\n- Tune `model_loader_extra_config` (`concurrency`, `distributed`, optional `memory_limit`).\n- Adjust `tensor_parallel_size`, `pipeline_parallel_size`, and `distributed_executor_backend` to match your GPU topology.\n\n### Running in-process (Python)\n```bash\ncd torch\npython forecaster_torch.py\n```\nThe ensemble uses `agents_vllm.Agent`, which loads via Run:ai from S3 per `config_vllm.py`.\n\n### Running vLLM server (optional)\n```bash\nvllm serve \\\n  --model s3://your-bucket/Llama-3-70B \\\n  --load-format runai_streamer \\\n  --tensor-parallel-size 8 \\\n  --pipeline-parallel-size 1 \\\n  --distributed-executor-backend mp \\\n  --gpu-memory-utilization 0.90 \\\n  --model-loader-extra-config '{\"concurrency\":32}'\n```\nThen point clients to the OpenAI-compatible endpoint (client wiring not included here).\n\n### AWS + S3 notes\n- Prefer IAM instance roles instead of static credentials. If needed, export `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_DEFAULT_REGION` (avoid committing them).\n- Keep S3 bucket in the same region as the instance.\n- For very large models, enable `swap_space_gb` in `config_vllm.py` to allow KV offload.\n\n### Performance tuning checklist\n- `tensor_parallel_size`: typically number of GPUs on the node.\n- `pipeline_parallel_size`: >1 only if the model still doesn’t fit and you span nodes.\n- `gpu_memory_utilization`: 0.90–0.95 on dedicated boxes.\n- `model_loader_extra_config.concurrency`: 16–64 depending on network/storage.\n- Pin GPUs with `CUDA_VISIBLE_DEVICES` if needed.\n\n### Logging & troubleshooting\n- `agents_vllm` logs TP/PP/backend and loader settings at startup.\n- If loads hang, check S3 IAM permissions/VPC endpoints; enable `NCCL_DEBUG=INFO` for multi-GPU issues.\n\n## Notes\n- Requirements pin `vllm[runai]` to 0.10.x for Run:ai streamer compatibility.\n- Sensitive files like `runpod.json` are removed/ignored; keep credentials out of the repo.\n*** End Patch
